1. Clusters    Time    Searches    Reducers
   ========================================
       6    00:58:37	  7             24
       9    28:00 	  13		36
       12	36:08	  13            48
2. Distances (Cluster Size: 12):
	50th: 4
	90th: 5
	95th: 5
3. Mean Processing Rate:
	6 clusters: .53 MB/s
	9 clusters: 2.1 MB/s
	12 clusters 1.6 MB/s
4. Speedup in comparison to 6 clusters:
	9: 52% speedup
	12: 38% speedup
	Conclusion: It parallelizes my work very well to an extent. While 9 clusters works faster than 6 and probably any cluster size below that, 12 clusters is too large and starts to show a decline in performance. This is an example of weak scaling.

5. Combiners in Hadoop act as a kind of "mini-Reducer" in that it decreases the number of (key, value) pairs that are passed as input into the Reduce phase of MapReduce. The crucial part of implementing this is reducing the intermediate values so that it will produce the same output for the Reduce phase. For Loader, our Mapper is simply an identity function, while our Reducer creates a list of destinations for each source node. We can add a Combiner to this step, combining the values of the individual (source, dest) pairs, since we will be creating a list that consists of all the values collected from the pairs. (What effect on performance?)

For BFS, our Reducer will 

6. Price per GB processed ($0.68 per hour rounded to nearest hour):
	6 clusters:
	9 clusters:
	12 clusters:
7. Dollars used: 44.20
